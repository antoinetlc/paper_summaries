<p align="center">
<b>Image-to-Image Translation with Conditional Adversarial Nets</b><br>
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros<br>
CVPR 2017<br>
<a href="https://phillipi.github.io/pix2pix/">Link to paper</a>
</p>

![Examples of image to image translation](https://github.com/antoinetlc/paper_summaries/blob/master/Papers/Image-to-Image_Translation_with_Conditional_Adversarial_Nets_Isola_et_al_CVPR17/Images/teaser.jpg)

### Context 

* The paper tackles the problem of image to image translation, i.e mapping an image to another image automatically without user intervention, using a set of paired images (input, target). Such problem often arises in computer graphics and vision. An example could be mapping a sketch to a drawing or changing a satellite image to a map.

### Novelty and contributions :

* The paper provides a general framework for image to image translation using conditonal generative adversarial networks (cGAN) that produces good results on a large variety of problems.

* It also studies the influence of the different architectures of the discriminator and generator and compare how changing the loss function affect the results.

### Proposed approach

In the paper, the authors propose to use conditional GANs instead of traditional GANs. They condition both the generator and the discriminator on the input image and show that cGANs are able to both capture low and high frequencies in images. The conditioning is very general as it is done using the discriminator direclty and does not depend on any application specific regularization term.

* `Generator architecture`. The generator architecture is very simple and based on a U-Net [1]. It is made of an encoder and a decoder that are mirror of each other with skip connections between the encoder and the decoder. Note that skip connections are very important to get results with sharp details (see figure 5 in the paper). Each layer is made of a regular convolution-batch normamlization-ReLU.

* Without any noise added inside the generator, the result would be deterministic. Instead they add dropout at several layers inside the generator to get some stochasticity in the output.

* `Discriminator architecture`. They propose to use a discriminator that focuses on classifying wether high frequency details are correct or not as the low frequencies can be captured using a L1 regularization (see loss function). In order to achieve this, the discriminator looks at all the NxN patches of an image and classifies wether each patch looks real or not. The output of the discriminator is the average of the probabilities for each patch. Results for various values of N are shown in figure 6 of the paper.

* `Reconstruction loss`. The reconstruction loss is a sum of two terms : the conditional adversarial loss and a reconstruction loss using L1 distance between the target image and the image the generator creates. Using a L1 distance instead of a L2 is prefered as L1 tends to gives less blurry results. The final loss function is :

![Loss function](https://github.com/antoinetlc/paper_summaries/blob/master/Papers/Image-to-Image_Translation_with_Conditional_Adversarial_Nets_Isola_et_al_CVPR17/Images/loss_function.png)

* `Difference between GANs and cGANs` In the GAN approach, a generator tries to generate a target Y from an input image X. Then the discriminator has to classify Y as real or fake (i.e belonging to the dataset or generated by the generator). The generator is trained to fool the discriminator and becomes better over time resulting in high quality results. Here the authors use a cGAN approach, meaning that both **the generator and the discriminator see the input image X**. For the image to image translation problem, it results in overall better results than the GAN framework.

### Results

* Please refer to the paper to see a wide variety of results on many different type of image to image translation problems (see section 4)

* Christopher Hesse also made an interactive online demo available here : https://affinelayer.com/pixsrv/

### References

* [1] O. Ronneberger, P. Fischer, and T. Brox.   U-net:  Convoluional networks for biomedical image segmentation. In MICCAI, pages 234â€“241. Springer, 2015.
