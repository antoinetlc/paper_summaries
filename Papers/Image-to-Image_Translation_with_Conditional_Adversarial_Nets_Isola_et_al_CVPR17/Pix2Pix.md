<p align="center">
<b>Image-to-Image Translation with Conditional Adversarial Nets</b><br>
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros<br>
CVPR 2017<br>
<a href="https://phillipi.github.io/pix2pix/">Link to paper</a>
</p>

![Examples of image to image translation](https://github.com/antoinetlc/paper_summaries/blob/master/Papers/Image-to-Image_Translation_with_Conditional_Adversarial_Nets_Isola_et_al_CVPR17/Images/teaser.jpg)

### Context 

* The paper tackles the problem of image translation, i.e mapping an image to another image automatically without user intervention, using a set of paired images (input, target). Such problem often arises in computer graphics and vision. An example could be mapping a sketch to a drawing or changing a satellite image to a map.

### Novelty and contributions :

* Provide a general framework for image to image translation using conditonal generative adversarial networks (cGAN) that produces sharp results and works on a large variety of problems.

* Study the influence of the different architectures of the discriminator and generator and compare how changing the loss function affect the results.

### How was it solved ?

In the paper the authors propose to use a conditional GAN instead of a traditional GAN. They show that cGANs produce results that both capture low and high frequencies in the image. 

* `Generator architecture` : the generator architecture is very simple and based on a U-Net [1]. It is made of an encoder and a decoder that are mirror of each other with skip connections between the encoder and the decoder. Note that skip connections are very important to get results with many details (see figure 5 in the paper).

* `Discriminator architecture` :  

* In the GAN approach, a generator tries to generate a target Y from an input image X. Then the discriminator has to classify Y as real or fake (i.e belonging to the dataset or generated by the generator). The generator is trained to fool the discriminator and becomes better over time resulting in high quality results. Here the authors use a cGAN approach, meaning that both **the generator and the discriminator see the input image X**. It results in overall better quality results than the GAN framework.

* `Reconstruction loss` : the reconstruction loss is a sum of two terms : the conditional adversarial loss and a reconstruction loss using L1 distance between the target image and the image the generator creates. They prefer using a L1 distance instead of L2 as L1 tends to gives less blurry results

![Loss function](https://github.com/antoinetlc/paper_summaries/blob/master/Papers/Image-to-Image_Translation_with_Conditional_Adversarial_Nets_Isola_et_al_CVPR17/Images/loss_function.png)

### Results


### References

* O. Ronneberger, P. Fischer, and T. Brox.   U-net:  Convoluional networks for biomedical image segmentation. In MICCAI, pages 234â€“241. Springer, 2015.
